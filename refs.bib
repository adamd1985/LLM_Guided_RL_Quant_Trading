@misc{vats2024surveyhumanaiteaminglarge,
      title={A Survey on Human-AI Teaming with Large Pre-Trained Models}, 
      author={Vanshika Vats and Marzia Binta Nizam and Minghao Liu and Ziyuan Wang and Richard Ho and Mohnish Sai Prasad and Vincent Titterton and Sai Venkat Malreddy and Riya Aggarwal and Yanwen Xu and Lei Ding and Jay Mehta and Nathan Grinnell and Li Liu and Sijia Zhong and Devanathan Nallur Gandamani and Xinyi Tang and Rohan Ghosalkar and Celeste Shen and Rachel Shen and Nafisa Hussain and Kesav Ravichandran and James Davis},
      year={2024},
      eprint={2403.04931},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.04931}, 
}

@article{Retzlaff2024HumanintheLoopRL,
  title={Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities},
  author={Charles Retzlaff and Srijita Das and Christabel Wayllace and Payam Mousavi and Mohammad Afshari and Tianpei Yang and Anna Saranti and Alessa Angerschmid and Matthew E. Taylor and Andreas Holzinger},
  journal={J. Artif. Intell. Res.},
  year={2024},
  volume={79},
  pages={359-415},
  url={https://api.semanticscholar.org/CorpusID:267448673}
}

@misc{mezghani2023thinkactunifiedpolicy,
      title={Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions}, 
      author={Lina Mezghani and Piotr Bojanowski and Karteek Alahari and Sainbayar Sukhbaatar},
      year={2023},
      eprint={2304.11063},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.11063}, 
}

@article{Pternea_2024,
   title={The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models},
   volume={80},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1.15960},
   DOI={10.1613/jair.1.15960},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Pternea, Moschoula and Singh, Prerna and Chakraborty, Abir and Oruganti, Yagna and Milletari, Mirco and Bapat, Sayli and Jiang, Kebei},
   year={2024},
   month=aug, pages={1525â€“1573} }

@misc{tan2024trueknowledgecomespractice,
      title={True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning}, 
      author={Weihao Tan and Wentao Zhang and Shanqi Liu and Longtao Zheng and Xinrun Wang and Bo An},
      year={2024},
      eprint={2401.14151},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.14151}, 
}

@misc{milani2022surveyexplainablereinforcementlearning,
      title={A Survey of Explainable Reinforcement Learning}, 
      author={Stephanie Milani and Nicholay Topin and Manuela Veloso and Fei Fang},
      year={2022},
      eprint={2202.08434},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.08434}, 
}

@misc{xie2024text2rewardrewardshapinglanguage,
      title={Text2Reward: Reward Shaping with Language Models for Reinforcement Learning}, 
      author={Tianbao Xie and Siheng Zhao and Chen Henry Wu and Yitao Liu and Qian Luo and Victor Zhong and Yanchao Yang and Tao Yu},
      year={2024},
      eprint={2309.11489},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.11489}, 
}

@inproceedings{gonen-etal-2023-demystifying,
    title = "Demystifying Prompts in Language Models via Perplexity Estimation",
    author = "Gonen, Hila  and
      Iyer, Srini  and
      Blevins, Terra  and
      Smith, Noah  and
      Zettlemoyer, Luke",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.679/",
    doi = "10.18653/v1/2023.findings-emnlp.679",
    pages = "10136--10148",
    abstract = "Language models can be prompted to perform a wide variety of tasks with zero- and few-shot in-context learning. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens. In this paper, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is predicted by the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt, the better it is able to perform the task, when considering reasonable prompts that are related to it. As part of our analysis, we also devise a method to automatically extend a small seed set of manually written prompts by paraphrasing with GPT3 and backtranslation. This larger set allows us to verify that perplexity is a strong predictor of the success of a prompt and we show that the lowest perplexity prompts are consistently effective."
}

@misc{huggingface2024perplexity,
  author    = "{Hugging Face}",
  title     = "{Perplexity in Language Models}",
  year      = {2024},
  url       = {https://huggingface.co/docs/transformers/en/perplexity},
  note      = {Accessed: 2024-06-05}
}

@misc{wang2024llmfactorextractingprofitablefactors,
      title={LLMFactor: Extracting Profitable Factors through Prompts for Explainable Stock Movement Prediction}, 
      author={Meiyun Wang and Kiyoshi Izumi and Hiroki Sakaji},
      year={2024},
      eprint={2406.10811},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10811}, 
}

@inproceedings{Demajo_2020, series={ACITY 2020},
   title={Explainable AI for Interpretable Credit Scoring},
   url={http://dx.doi.org/10.5121/csit.2020.101516},
   DOI={10.5121/csit.2020.101516},
   booktitle={Computer Science &amp; Information Technology (CS &amp; IT)},
   publisher={AIRCC Publishing Corporation},
   author={Demajo, Lara Marie and Vella, Vince and Dingli, Alexiei},
   year={2020},
   month=nov, collection={ACITY 2020} }

@article{xai_Khawaga_2023,
author = {El-Khawaga, Ghada and Elzeki, Omar and Abuelkheir, Mervat and Reichert, Manfred},
year = {2023},
month = {03},
pages = {1670},
title = {Evaluating Explainable Artificial Intelligence Methods Based on Feature Elimination: A Functionality-Grounded Approach},
volume = {12},
journal = {Electronics},
doi = {10.3390/electronics12071670}
}